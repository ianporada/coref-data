{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "\n",
    "from stanza.models.constituency.tree_reader import read_trees\n",
    "from stanza.models.constituency.parse_tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f663fdad194cc4823d4e42d1469dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563a5774d8914fed8b88dd38328588b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31add15126f4a59955a00664c5616c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6414d455e0fb4b338511d5d5303f6deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f13813ee6441aba5f1ab24fec6fbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057901916def4240a762fc372407139c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e03691b72ba4dee928bcc20fe4e03e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conll_data = datasets.load_dataset(\"coref-data/conll2012_indiscrim\", \"english_v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': [{'id': Value(dtype='int64', id=None),\n",
       "   'misc': {'parse_tree': Value(dtype='string', id=None)},\n",
       "   'speaker': Value(dtype='string', id=None),\n",
       "   'text': Value(dtype='string', id=None),\n",
       "   'tokens': [{'deprel': Value(dtype='string', id=None),\n",
       "     'head': Value(dtype='int64', id=None),\n",
       "     'id': Value(dtype='int64', id=None),\n",
       "     'text': Value(dtype='string', id=None),\n",
       "     'upos': Value(dtype='string', id=None),\n",
       "     'xpos': Value(dtype='string', id=None)}]}],\n",
       " 'id': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'coref_chains': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'genre': Value(dtype='string', id=None),\n",
       " 'meta_data': {'comment': Value(dtype='string', id=None)}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_data[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_noun_phrases(node: Tree):\n",
    "    if node.label == \"NP\" or node.label == \"NML\":\n",
    "        yield node\n",
    "    if not node.is_leaf():\n",
    "        for child in node.children:\n",
    "            yield from get_all_noun_phrases(child)\n",
    "\n",
    "\n",
    "def get_all_noun_phrase_indices(tree):\n",
    "    # map words to indices and get all noun phrases\n",
    "    str_indices = map(str, range(len(tree)))\n",
    "    index_tree = tree.replace_words(str_indices)\n",
    "    return get_all_noun_phrases(index_tree)\n",
    "\n",
    "\n",
    "def get_np_mentions(sent_i, sentence):\n",
    "    tokens = sentence[\"tokens\"]\n",
    "    parse_tree = sentence[\"misc\"][\"parse_tree\"]\n",
    "\n",
    "    if not parse_tree:\n",
    "        return\n",
    "\n",
    "    trees = read_trees(parse_tree)\n",
    "    assert len(trees) == 1\n",
    "    tree: Tree = trees[0]\n",
    "\n",
    "    assert len(tree) == len(tokens)\n",
    "\n",
    "    for np_node in get_all_noun_phrase_indices(tree):\n",
    "        leaves = np_node.leaf_labels()\n",
    "        yield [sent_i, int(leaves[0]), int(leaves[-1])]\n",
    "\n",
    "\n",
    "def add_singletons(example):\n",
    "    sentences = example[\"sentences\"]\n",
    "    coref_chains = example[\"coref_chains\"]\n",
    "\n",
    "    singleton_mentions = []\n",
    "    for sent_i, sentence in enumerate(sentences):\n",
    "        for m in get_np_mentions(sent_i, sentence):\n",
    "            singleton_mentions.append(m)\n",
    "        \n",
    "        for i, t in enumerate(sentence[\"tokens\"]):\n",
    "            if t[\"xpos\"] in [\"PRP\", \"PRP$\"] or t[\"xpos\"][0] == \"V\" or t[\"xpos\"] in [\"WP\", \"WP$\", \"WDT\", \"WRB\"] or t[\"xpos\"][0] == \"N\" :\n",
    "                singleton_mentions.append([sent_i, i, i])\n",
    "\n",
    "    return {\"singleton_mentions\": singleton_mentions}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbad0363a774a9cba6ec22c7e7039c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deaba0ad28346438f2874d0a86f94ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2680f9ef669c4606ad87de810d9e2d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "singleton_data = conll_data.map(add_singletons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "155225 155558 1865019 0.997859319353553 0.08322971508601253\n",
      "validation\n",
      "19083 19155 233835 0.9962411902897416 0.08160882673680159\n",
      "test\n",
      "19709 19764 242523 0.997217162517709 0.08126651905180127\n"
     ]
    }
   ],
   "source": [
    "# calculate precision and recall\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    recalled = 0\n",
    "    total_coref = 0\n",
    "    total_singleton = 0\n",
    "\n",
    "    for example in singleton_data[split]:\n",
    "        coref_chains = example[\"coref_chains\"]\n",
    "        singleton_mentions = example[\"singleton_mentions\"]\n",
    "\n",
    "        coref_mentions = set()\n",
    "        for c in coref_chains:\n",
    "            for m in c:\n",
    "                coref_mentions.add(tuple(m))\n",
    "\n",
    "        singleton_mentions = set([tuple(m) for m in singleton_mentions])\n",
    "\n",
    "        sent2singletons = collections.defaultdict(list)\n",
    "        for m in singleton_mentions:\n",
    "            sent2singletons[m[0]].append(m)\n",
    "\n",
    "        # for all mentions in the same sentence, find if any mentions start one word later\n",
    "        for same_sent_ments in sent2singletons.values():\n",
    "            for m in same_sent_ments:\n",
    "                for n in same_sent_ments:\n",
    "                    if m[2] == n[1] - 1 or m[2] == n[1] - 2:\n",
    "                        singleton_mentions.add((m[0], m[1], n[2]))\n",
    "\n",
    "        total_coref += len(coref_mentions)\n",
    "        total_singleton += len(singleton_mentions)\n",
    "\n",
    "        recalled += len(singleton_mentions.intersection(coref_mentions))\n",
    "        \n",
    "    print(split)\n",
    "    print(recalled, total_coref, total_singleton, float(recalled) / total_coref, float(recalled) / total_singleton)\n",
    "    # what percent of coref are in singleton? what percent of singleton are in coref?\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate singleton mentions using parse\n",
    "\n",
    "# All noun phrases with distinct headwords are extracted from previously treebanked data\n",
    "# Whenever head-sharing NPs are nested, the largest logical span is used in co-reference (see 2.4.1).\n",
    "## Head-sharing NPs are two (or more) extracted entities, the shorter one(s) contained within the span of the longer,\n",
    "## sharing the same content word as their headword.\n",
    "## In such cases, the longest logical span should be used in co-reference with other mentions. \n",
    "\n",
    "# Possessive proper nouns (Fred's) are extracted from the treebanked data;\n",
    "# however, possessive pronouns (his) must be manually extracted by the annotator and added to the list of mentions\n",
    "\n",
    "# Proper noun PreMods can be co-referenced to existing noun phrases and/or other proper PreMods,\n",
    "# and should be manually extracted by the annotator and added to the list of mentions.\n",
    "# Non-proper and adjectival premodifiers are not eligible for co-reference (see 2.3).\n",
    "## Premodifiers must be proper nouns\n",
    "## Pre-modifying dates and monetary amounts are also eligible for co-reference\n",
    "## Acronymic premodifiers should be co-referenced unless they refer to nationality\n",
    "\n",
    "# Only the single-word head of the verb phrase is included in the span,\n",
    "# even in cases where the entire verb phrase is the logical co-referent.\n",
    "\n",
    "# Appositives: only the whole span is linked for IDENT\n",
    "\n",
    "# Partitives: all and both can corefer in nesting\n",
    "\n",
    "# in, at, to, from cannot be metonyms\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
